# LLM-based Recommender System Configuration

# Data Configuration
data:
  user_sequences_path: "/space/mcdonald-syn01/1/projects/jsawant/llm_recommender/dataset/filtered_user_sequences.jsonl.gz"
  item_metadata_path: "/space/mcdonald-syn01/1/projects/jsawant/llm_recommender/dataset/filtered_Kindle_Store.jsonl.gz"
  train_ratio: 0.8
  val_ratio: 0.1
  min_sequence_length: 3
  max_sequence_length: 50
  seed: 42
  max_train_sequences: 100000  # Truncate to this many training sequences (None = no truncation)

# Model Configuration
model:
  # Options:
  # GPT-2: "gpt2" (124M), "gpt2-medium" (355M), "gpt2-large" (762M), "gpt2-xl" (1.5B)
  # Qwen: "Qwen/Qwen2-0.5B" (500M), "Qwen/Qwen2-1.5B" (1.5B), "Qwen/Qwen2-7B" (7B)
  # For 500M: use "Qwen/Qwen2-0.5B" or "gpt2-large" (762M)
  # For 1B: use "Qwen/Qwen2-1.5B" (closest to 1B) or "gpt2-xl" (1.5B)
  base_llm: "Qwen/Qwen2-0.5B"  # Change to "Qwen/Qwen2-0.5B" for 500M or "Qwen/Qwen2-1.5B" for ~1B
  embedding_dim: 128  # User/item embedding dimension (will be projected to LLM hidden size)
  freeze_llm_stage_a: false
  random_init_stage_a_llm: false  # If true, initialize LLM randomly instead of loading pretrained weights
  
# Embedding Configuration
embeddings:
  lambda_c: 0.001  # Collaborative embedding regularization

# Stage A: Embedding Pretraining
stage_a:
  epochs: 10
  batch_size: 32
  learning_rate: 5e-4
  weight_decay: 0.001
  max_grad_norm: 1.0
  
  # Loss weights
  loss_weights:
    collaborative: 1.0  # Cross-entropy loss weight
    cf_bpr: 1.0  # BPR loss weight
    regularization: 0.0  # Set to 0.0 to disable regularization loss
  
  # Collaborative loss settings
  collaborative:
    negative_samples: 30  # Recommended: 20-50 for large item spaces (349K items)
                           # More negatives = better gradient signal but slower training
    use_bpr_loss: true  # Enable BPR loss (both autoregressive and single-target modes)

  
# Training Configuration
training:
  device: "cuda:0"
  mixed_precision: "no"  # "no", "fp16", or "bf16"
  logging_steps: 500
  eval_steps: 10000
  save_steps: 10000

